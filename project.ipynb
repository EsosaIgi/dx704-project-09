{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 9 Project\n",
        "\n",
        "This week's project will build an email spam classifier based on the Enron email data set.\n",
        "You will perform your own feature extraction, and use naive Bayes to estimate the probability that a particular email is spam or not.\n",
        "Finally, you will review the tradeoffs from different thresholds for automatically sending emails to the junk folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBdILvlviZs2"
      },
      "source": [
        "The full project description and a template notebook are available on GitHub: [Project 9 Materials](https://github.com/bu-cds-dx704/dx704-project-09).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRDlXZBVd2aR"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1nEl0_Khm_"
      },
      "source": [
        "## Part 1: Download Data Set\n",
        "\n",
        "We will be using the Enron spam data set as prepared in this GitHub repository.\n",
        "\n",
        "https://github.com/MWiechmann/enron_spam_data\n",
        "\n",
        "You may need to download this differently depending on your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwpoSzUxKmG9",
        "outputId": "3ace62f1-c32a-462a-d538-36f3638b7dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-01 23:15:32--  https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip [following]\n",
            "--2025-11-01 23:15:32--  https://raw.githubusercontent.com/MWiechmann/enron_spam_data/refs/heads/master/enron_spam_data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15642124 (15M) [application/zip]\n",
            "Saving to: ‘enron_spam_data.zip.1’\n",
            "\n",
            "enron_spam_data.zip 100%[===================>]  14.92M  92.0MB/s    in 0.2s    \n",
            "\n",
            "2025-11-01 23:15:33 (92.0 MB/s) - ‘enron_spam_data.zip.1’ saved [15642124/15642124]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/MWiechmann/enron_spam_data/raw/refs/heads/master/enron_spam_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "EfCir3ILLv8z"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "9gn-4hUzLywO",
        "outputId": "f810652e-7829-44dd-e842-137281f53be2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>Subject</th>\n",
              "      <th>Message</th>\n",
              "      <th>Spam/Ham</th>\n",
              "      <th>Date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>christmas tree farm pictures</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>vastar resources , inc .</td>\n",
              "      <td>gary , production from the high island larger ...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>calpine daily gas nomination</td>\n",
              "      <td>- calpine daily gas nomination 1 . doc</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>re : issue</td>\n",
              "      <td>fyi - see note below - already done .\\nstella\\...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>meter 7268 nov allocation</td>\n",
              "      <td>fyi .\\n- - - - - - - - - - - - - - - - - - - -...</td>\n",
              "      <td>ham</td>\n",
              "      <td>1999-12-14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33711</th>\n",
              "      <td>33711</td>\n",
              "      <td>= ? iso - 8859 - 1 ? q ? good _ news _ c = eda...</td>\n",
              "      <td>hello , welcome to gigapharm onlinne shop .\\np...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33712</th>\n",
              "      <td>33712</td>\n",
              "      <td>all prescript medicines are on special . to be...</td>\n",
              "      <td>i got it earlier than expected and it was wrap...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33713</th>\n",
              "      <td>33713</td>\n",
              "      <td>the next generation online pharmacy .</td>\n",
              "      <td>are you ready to rock on ? let the man in you ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33714</th>\n",
              "      <td>33714</td>\n",
              "      <td>bloow in 5 - 10 times the time</td>\n",
              "      <td>learn how to last 5 - 10 times longer in\\nbed ...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33715</th>\n",
              "      <td>33715</td>\n",
              "      <td>dear sir , i am interested in it</td>\n",
              "      <td>hi : )\\ndo you need some softwares ? i can giv...</td>\n",
              "      <td>spam</td>\n",
              "      <td>2005-07-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>33716 rows × 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Message ID                                            Subject  \\\n",
              "0               0                       christmas tree farm pictures   \n",
              "1               1                           vastar resources , inc .   \n",
              "2               2                       calpine daily gas nomination   \n",
              "3               3                                         re : issue   \n",
              "4               4                          meter 7268 nov allocation   \n",
              "...           ...                                                ...   \n",
              "33711       33711  = ? iso - 8859 - 1 ? q ? good _ news _ c = eda...   \n",
              "33712       33712  all prescript medicines are on special . to be...   \n",
              "33713       33713              the next generation online pharmacy .   \n",
              "33714       33714                     bloow in 5 - 10 times the time   \n",
              "33715       33715                   dear sir , i am interested in it   \n",
              "\n",
              "                                                 Message Spam/Ham        Date  \n",
              "0                                                    NaN      ham  1999-12-10  \n",
              "1      gary , production from the high island larger ...      ham  1999-12-13  \n",
              "2                 - calpine daily gas nomination 1 . doc      ham  1999-12-14  \n",
              "3      fyi - see note below - already done .\\nstella\\...      ham  1999-12-14  \n",
              "4      fyi .\\n- - - - - - - - - - - - - - - - - - - -...      ham  1999-12-14  \n",
              "...                                                  ...      ...         ...  \n",
              "33711  hello , welcome to gigapharm onlinne shop .\\np...     spam  2005-07-29  \n",
              "33712  i got it earlier than expected and it was wrap...     spam  2005-07-29  \n",
              "33713  are you ready to rock on ? let the man in you ...     spam  2005-07-30  \n",
              "33714  learn how to last 5 - 10 times longer in\\nbed ...     spam  2005-07-30  \n",
              "33715  hi : )\\ndo you need some softwares ? i can giv...     spam  2005-07-31  \n",
              "\n",
              "[33716 rows x 5 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# pandas can read the zip file directly\n",
        "enron_spam_data = pd.read_csv(\"enron_spam_data.zip\")\n",
        "enron_spam_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYypb_fJWF_A",
        "outputId": "17478b00-1c10-4026-a42b-dcc8a368eab5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(0.5092834262664611)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(enron_spam_data[\"Spam/Ham\"] == \"spam\").mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 2: Design a Feature Extractor\n",
        "\n",
        "Design a feature extractor for this data set and write out two files of features based on the text.\n",
        "Don't forget that both the Subject and Message columns are relevant sources of text data.\n",
        "For each email, you should count the number of repetitions of each feature present.\n",
        "The auto-grader will assume that you are using a multinomial distribution in the following problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-CF6wtn_VRjp"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import re, json\n",
        "from collections import Counter\n",
        "\n",
        "TOKEN_RE = re.compile(r\"[A-Za-z0-9']+\")\n",
        "\n",
        "def tokenize(text: str):\n",
        "    return [t.lower() for t in TOKEN_RE.findall(text or \"\")]\n",
        "\n",
        "def extract_handcrafted_counts(subject: str, message: str):\n",
        "    \"\"\"Return a dict[str,int] of multinomial-friendly counts.\"\"\"\n",
        "    subj = subject or \"\"\n",
        "    msg  = message or \"\"\n",
        "\n",
        "    # Tokenize\n",
        "    st = tokenize(subj)\n",
        "    mt = tokenize(msg)\n",
        "\n",
        "    # Counts (mildly emphasize subject by repeating each token once)\n",
        "    counts = Counter(st + st)    # 2x subject\n",
        "    counts.update(mt)\n",
        "\n",
        "    # Light engineered integer features (still counts)\n",
        "    s = subj + \"\\n\" + msg\n",
        "    counts.update({\n",
        "        \"__EXCLAIMS__\": s.count(\"!\"),\n",
        "        \"__DOLLARS__\": s.count(\"$\"),\n",
        "        \"__PERCENTS__\": s.count(\"%\"),\n",
        "        \"__URLS__\": len(re.findall(r\"(https?://|www\\.)\\S+\", s, flags=re.I)),\n",
        "        \"__EMAILS__\": len(re.findall(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\", s)),\n",
        "        \"__UPPER_WORDS__\": sum(1 for w in re.findall(r\"\\b[A-Z]{2,}\\b\", s)),\n",
        "        \"__LEN_BUCKET_100__\": len(s) // 100,\n",
        "        \"__TOK_BUCKET_50__\":  (len(st)+len(mt)) // 50,\n",
        "        \"cue_subject_re\": int(subj.lower().startswith(\"re:\")),\n",
        "        \"cue_subject_fwd\": int(subj.lower().startswith((\"fwd:\", \"fw:\"))),\n",
        "        \"cue_contains_free\": int(\"free\" in s.lower()),\n",
        "        \"cue_contains_click\": int(\"click\" in s.lower()),\n",
        "        \"cue_contains_offer\": int(\"offer\" in s.lower()),\n",
        "    })\n",
        "    return {k:int(v) for k,v in counts.items() if int(v) > 0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g90ug-qYVWI2"
      },
      "source": [
        "Assign a row to the test data set if `Message ID % 30 == 0` and assign it to the training data set otherwise.\n",
        "Write two files, \"train-features.tsv\" and \"test-features.tsv\" with two columns, Message ID and features_json.\n",
        "The features_json column should contain a JSON dictionary where the keys are your feature names and the values are integer feature values.\n",
        "This will give us a sparse feature representation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t7AjXVlXUpaR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved train-features.tsv (rows=32542) and test-features.tsv (rows=1174)\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import pandas as pd, numpy as np, zipfile, io, hashlib, re, json\n",
        "from pathlib import Path\n",
        "\n",
        "ZIP_PATH = Path(\"enron_spam_data.zip\")\n",
        "\n",
        "def load_enron_zip(zip_path=ZIP_PATH) -> pd.DataFrame:\n",
        "    \"\"\"Return DataFrame with columns Subject, Message, Spam/Ham, Date.\"\"\"\n",
        "    assert zip_path.exists(), \"enron_spam_data.zip not found in the working directory.\"\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        # The zip contains a single CSV inside; find it:\n",
        "        inner = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "        assert inner, \"CSV not found inside enron_spam_data.zip\"\n",
        "        with z.open(inner[0]) as f:\n",
        "            df = pd.read_csv(io.TextIOWrapper(f, encoding=\"utf-8\"))\n",
        "    # Normalize expected columns per repo README (Subject, Message, Spam/Ham, Date)\n",
        "    ren = {c.lower(): c for c in df.columns}\n",
        "    need = [\"subject\",\"message\",\"spam/ham\",\"date\"]\n",
        "    missing = [x for x in need if x not in ren]\n",
        "    if missing:\n",
        "        raise ValueError(f\"CSV missing expected columns: {missing}; found={list(df.columns)}\")\n",
        "    return df.rename(columns={\n",
        "        ren[\"subject\"]:\"Subject\",\n",
        "        ren[\"message\"]:\"Message\",\n",
        "        ren[\"spam/ham\"]:\"Spam/Ham\",\n",
        "        ren[\"date\"]:\"Date\",\n",
        "    })[[\"Subject\",\"Message\",\"Spam/Ham\",\"Date\"]]\n",
        "\n",
        "def ensure_message_id(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create a stable Message ID if the CSV didn’t include one.\"\"\"\n",
        "    if \"Message ID\" in df.columns:\n",
        "        return df\n",
        "    # Use a stable 32-bit hash of Subject+Message+Date\n",
        "    def mk_id(row):\n",
        "        key = (str(row[\"Subject\"]) + \"\\n\" + str(row[\"Message\"]) + \"\\n\" + str(row[\"Date\"])).encode(\"utf-8\", \"ignore\")\n",
        "        return int(hashlib.md5(key).hexdigest()[:8], 16)\n",
        "    out = df.copy()\n",
        "    out[\"Message ID\"] = out.apply(mk_id, axis=1)\n",
        "    return out\n",
        "\n",
        "# 1) Load and normalize\n",
        "all_df = load_enron_zip()\n",
        "all_df = ensure_message_id(all_df)\n",
        "all_df = all_df[[\"Message ID\",\"Subject\",\"Message\"]].copy()\n",
        "all_df[\"Subject\"] = all_df[\"Subject\"].astype(str)\n",
        "all_df[\"Message\"] = all_df[\"Message\"].astype(str)\n",
        "\n",
        "# 2) Build features_json per row\n",
        "rows = []\n",
        "for _, r in all_df.iterrows():\n",
        "    feats = extract_handcrafted_counts(r[\"Subject\"], r[\"Message\"])\n",
        "    rows.append({\"Message ID\": r[\"Message ID\"], \"features_json\": json.dumps(feats, ensure_ascii=False)})\n",
        "\n",
        "feat_df = pd.DataFrame(rows)\n",
        "\n",
        "# 3) Split rule: test if (Message ID % 30 == 0)\n",
        "def id_to_int(x):\n",
        "    try:\n",
        "        return int(x)\n",
        "    except Exception:\n",
        "        # last 8 hex of md5 to int — should be rare given we created int above\n",
        "        return int(hashlib.md5(str(x).encode(\"utf-8\")).hexdigest()[:8], 16)\n",
        "\n",
        "feat_df[\"_id_int\"] = feat_df[\"Message ID\"].apply(id_to_int)\n",
        "test_mask = (feat_df[\"_id_int\"] % 30 == 0)\n",
        "\n",
        "train_out = feat_df.loc[~test_mask, [\"Message ID\",\"features_json\"]].reset_index(drop=True)\n",
        "test_out  = feat_df.loc[ test_mask, [\"Message ID\",\"features_json\"]].reset_index(drop=True)\n",
        "\n",
        "# 4) Save\n",
        "train_out.to_csv(\"train-features.tsv\", sep=\"\\t\", index=False)\n",
        "test_out.to_csv(\"test-features.tsv\",  sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Saved train-features.tsv (rows={len(train_out)}) and test-features.tsv (rows={len(test_out)})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAEYBd7WUrC0"
      },
      "source": [
        "Submit \"train-features.tsv\" and \"test-features.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrLU0aIaNB7"
      },
      "source": [
        "Hint: these features will be graded based on the test accuracy of a logistic regression based on the training features.\n",
        "This is to make sure that your feature set is not degenerate; you do not need to compute this regression yourself.\n",
        "You can separately assess your feature quality based on your results in part 6."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_PhU4d5vEFX"
      },
      "source": [
        "## Part 3: Compute Conditional Probabilities\n",
        "\n",
        "Based on your training data, compute appropriate conditional probabilities for use with naïve Bayes.\n",
        "Use of additive smoothing with $\\alpha=1$ to avoid zeros.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3MKi6er-Vde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed conditional probabilities for 154675 features.\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import zipfile, io, hashlib\n",
        "\n",
        "def load_enron_zip(zip_path=Path(\"enron_spam_data.zip\")) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load the official CSV from enron_spam_data.zip and return a DataFrame with:\n",
        "    ['Subject','Message','Spam/Ham','Date']\n",
        "    \"\"\"\n",
        "    assert zip_path.exists(), \"enron_spam_data.zip not found in the working directory.\"\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        inner = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "        assert inner, \"CSV not found inside enron_spam_data.zip\"\n",
        "        with z.open(inner[0]) as f:\n",
        "            df = pd.read_csv(io.TextIOWrapper(f, encoding=\"utf-8\"))\n",
        "    ren = {c.lower(): c for c in df.columns}\n",
        "    need = [\"subject\",\"message\",\"spam/ham\",\"date\"]\n",
        "    missing = [x for x in need if x not in ren]\n",
        "    if missing:\n",
        "        raise ValueError(f\"CSV missing expected columns: {missing}; found={list(df.columns)}\")\n",
        "    return df.rename(columns={\n",
        "        ren[\"subject\"]:\"Subject\",\n",
        "        ren[\"message\"]:\"Message\",\n",
        "        ren[\"spam/ham\"]:\"Spam/Ham\",\n",
        "        ren[\"date\"]:\"Date\",\n",
        "    })[[\"Subject\",\"Message\",\"Spam/Ham\",\"Date\"]]\n",
        "\n",
        "def ensure_message_id(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Synthesize the same Message ID used earlier (stable 32-bit hash of Subject+Message+Date).\n",
        "    \"\"\"\n",
        "    if \"Message ID\" in df.columns:\n",
        "        return df\n",
        "    def mk_id(row):\n",
        "        key = (str(row[\"Subject\"]) + \"\\n\" + str(row[\"Message\"]) + \"\\n\" + str(row[\"Date\"])).encode(\"utf-8\", \"ignore\")\n",
        "        return int(hashlib.md5(key).hexdigest()[:8], 16)\n",
        "    out = df.copy()\n",
        "    out[\"Message ID\"] = out.apply(mk_id, axis=1)\n",
        "    return out\n",
        "\n",
        "train_feat_path = Path(\"train-features.tsv\")\n",
        "assert train_feat_path.exists(), \"train-features.tsv not found. Run Part 2 first.\"\n",
        "\n",
        "train_feats = pd.read_csv(train_feat_path, sep=\"\\t\")\n",
        "assert {\"Message ID\",\"features_json\"}.issubset(train_feats.columns), \"train-features.tsv missing required columns.\"\n",
        "\n",
        "# Load raw data (to get labels), synthesize IDs, and join labels\n",
        "raw_df = ensure_message_id(load_enron_zip())\n",
        "label_map = raw_df.set_index(\"Message ID\")[\"Spam/Ham\"].to_dict()\n",
        "\n",
        "# Keep only rows in our training features (merge by ID)\n",
        "train_feats[\"Label\"] = train_feats[\"Message ID\"].map(label_map)\n",
        "missing = train_feats[\"Label\"].isna().sum()\n",
        "if missing:\n",
        "    # If some IDs don't match (shouldn't happen if IDs were built the same way),\n",
        "    # drop those to avoid contaminating counts.\n",
        "    train_feats = train_feats.dropna(subset=[\"Label\"]).reset_index(drop=True)\n",
        "\n",
        "#  aggregate counts per class (Multinomial) \n",
        "alpha = 1.0  # Laplace smoothing\n",
        "\n",
        "ham_counts  = Counter()\n",
        "spam_counts = Counter()\n",
        "\n",
        "for _, row in train_feats.iterrows():\n",
        "    feats = json.loads(row[\"features_json\"]) if isinstance(row[\"features_json\"], str) else {}\n",
        "    if row[\"Label\"].strip().lower() == \"ham\":\n",
        "        ham_counts.update({k:int(v) for k,v in feats.items() if int(v) > 0})\n",
        "    else:\n",
        "        spam_counts.update({k:int(v) for k,v in feats.items() if int(v) > 0})\n",
        "\n",
        "# Vocabulary is union of features observed in training\n",
        "vocab = sorted(set(ham_counts.keys()) | set(spam_counts.keys()))\n",
        "V = len(vocab)\n",
        "\n",
        "# Totals per class\n",
        "ham_total  = sum(ham_counts.values())\n",
        "spam_total = sum(spam_counts.values())\n",
        "\n",
        "# compute conditional probabilities with smoothing \n",
        "# P(feature | class) = (count + α) / (total_count + α * V)\n",
        "den_ham  = ham_total  + alpha * V\n",
        "den_spam = spam_total + alpha * V\n",
        "\n",
        "ham_prob  = {}\n",
        "spam_prob = {}\n",
        "for f in vocab:\n",
        "    hc = ham_counts.get(f, 0)\n",
        "    sc = spam_counts.get(f, 0)\n",
        "    ham_prob[f]  = (hc + alpha) / den_ham\n",
        "    spam_prob[f] = (sc + alpha) / den_spam\n",
        "\n",
        "print(f\"Computed conditional probabilities for {V} features.\")\n",
        "# Keep these around for the save cell\n",
        "feature_vocab = vocab\n",
        "ham_probability  = ham_prob\n",
        "spam_probability = spam_prob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbDJfLCdVfHh"
      },
      "source": [
        "Save the conditional probabilities in a file \"feature-probabilities.tsv\" with columns feature, ham_probability and spam_probability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kTVFW327VsOC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved feature-probabilities.tsv with 154675 rows (columns: feature, ham_probability, spam_probability).\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import pandas as pd\n",
        "\n",
        "assert 'feature_vocab' in globals() and 'ham_probability' in globals() and 'spam_probability' in globals(), \\\n",
        "    \"Run the previous Part 3 cell first.\"\n",
        "\n",
        "out_df = pd.DataFrame({\n",
        "    \"feature\": feature_vocab,\n",
        "    \"ham_probability\":  [ham_probability[f]  for f in feature_vocab],\n",
        "    \"spam_probability\": [spam_probability[f] for f in feature_vocab],\n",
        "})\n",
        "\n",
        "out_df.to_csv(\"feature-probabilities.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"Saved feature-probabilities.tsv with {len(out_df)} rows (columns: feature, ham_probability, spam_probability).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ip-k6K-hVt6q"
      },
      "source": [
        "Submit \"feature-probabilities.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuQpZbILYqNd"
      },
      "source": [
        "## Part 4: Implement a Naïve Bayes Classifier\n",
        "\n",
        "Implement a naïve Bayes classifier based on your previous feature probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jkZeyZgsWr5-"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import json, io, zipfile, hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# Load conditional probabilities \n",
        "fp_path = Path(\"feature-probabilities.tsv\")\n",
        "assert fp_path.exists(), \"feature-probabilities.tsv not found. Run Part 3 first.\"\n",
        "fp = pd.read_csv(fp_path, sep=\"\\t\")\n",
        "assert {\"feature\",\"ham_probability\",\"spam_probability\"}.issubset(fp.columns)\n",
        "\n",
        "# Dicts for fast lookup\n",
        "p_ham = dict(zip(fp[\"feature\"].astype(str), fp[\"ham_probability\"].astype(float)))\n",
        "p_spam = dict(zip(fp[\"feature\"].astype(str), fp[\"spam_probability\"].astype(float)))\n",
        "\n",
        "p_unk_ham  = float(fp[\"ham_probability\"].min())\n",
        "p_unk_spam = float(fp[\"spam_probability\"].min())\n",
        "\n",
        "log_p_ham  = {k: np.log(v + 1e-12) for k,v in p_ham.items()}\n",
        "log_p_spam = {k: np.log(v + 1e-12) for k,v in p_spam.items()}\n",
        "log_punk_ham  = float(np.log(p_unk_ham  + 1e-12))\n",
        "log_punk_spam = float(np.log(p_unk_spam + 1e-12))\n",
        "\n",
        "#  Class priors from training labels (ham/spam) \n",
        "def load_enron_zip(zip_path=Path(\"enron_spam_data.zip\")) -> pd.DataFrame:\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        inner = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "        assert inner, \"CSV not found inside enron_spam_data.zip\"\n",
        "        with z.open(inner[0]) as f:\n",
        "            df = pd.read_csv(io.TextIOWrapper(f, encoding=\"utf-8\"))\n",
        "    ren = {c.lower(): c for c in df.columns}\n",
        "    df = df.rename(columns={\n",
        "        ren[\"subject\"]:\"Subject\",\n",
        "        ren[\"message\"]:\"Message\",\n",
        "        ren[\"spam/ham\"]:\"Spam/Ham\",\n",
        "        ren[\"date\"]:\"Date\",\n",
        "    })[[\"Subject\",\"Message\",\"Spam/Ham\",\"Date\"]]\n",
        "    return df\n",
        "\n",
        "def ensure_message_id(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"Message ID\" in df.columns:\n",
        "        return df\n",
        "    def mk_id(row):\n",
        "        key = (str(row[\"Subject\"]) + \"\\n\" + str(row[\"Message\"]) + \"\\n\" + str(row[\"Date\"])).encode(\"utf-8\",\"ignore\")\n",
        "        return int(hashlib.md5(key).hexdigest()[:8], 16)\n",
        "    out = df.copy()\n",
        "    out[\"Message ID\"] = out.apply(mk_id, axis=1)\n",
        "    return out\n",
        "\n",
        "train_feats = pd.read_csv(\"train-features.tsv\", sep=\"\\t\")\n",
        "assert {\"Message ID\",\"features_json\"}.issubset(train_feats.columns)\n",
        "\n",
        "raw = ensure_message_id(load_enron_zip())\n",
        "label_map = raw.set_index(\"Message ID\")[\"Spam/Ham\"].to_dict()\n",
        "\n",
        "# Join labels only for rows we have features for (priors should reflect training subset)\n",
        "train_feats[\"Label\"] = train_feats[\"Message ID\"].map(label_map)\n",
        "train_feats = train_feats.dropna(subset=[\"Label\"]).reset_index(drop=True)\n",
        "\n",
        "n_ham  = int((train_feats[\"Label\"].str.lower()==\"ham\").sum())\n",
        "n_spam = int((train_feats[\"Label\"].str.lower()==\"spam\").sum())\n",
        "n_tot  = max(1, n_ham + n_spam)\n",
        "\n",
        "prior_ham  = n_ham  / n_tot\n",
        "prior_spam = n_spam / n_tot\n",
        "log_prior_ham  = float(np.log(prior_ham  + 1e-12))\n",
        "log_prior_spam = float(np.log(prior_spam + 1e-12))\n",
        "\n",
        "# Predict log-probabilities given a features dict {feature: count} \n",
        "def predict_log_proba_from_counts(counts: dict) -> tuple[float, float]:\n",
        "    # log P(c) + sum_j count_j * log P(feature_j | c)\n",
        "    lh = log_prior_ham\n",
        "    ls = log_prior_spam\n",
        "    for f, c in counts.items():\n",
        "        if not c:\n",
        "            continue\n",
        "        lp_h = log_p_ham.get(f,  log_punk_ham)\n",
        "        lp_s = log_p_spam.get(f, log_punk_spam)\n",
        "        lh += int(c) * lp_h\n",
        "        ls += int(c) * lp_s\n",
        "    return lh, ls\n",
        "\n",
        "def log_to_prob_pair(lh: float, ls: float) -> tuple[float, float]:\n",
        "    m = max(lh, ls)\n",
        "    eh = np.exp(lh - m)\n",
        "    es = np.exp(ls - m)\n",
        "    z = eh + es\n",
        "    return float(eh / z), float(es / z)  # (p_ham, p_spam)\n",
        "\n",
        "def predict_proba_features_json(feat_json: str) -> tuple[float, float]:\n",
        "    counts = json.loads(feat_json) if isinstance(feat_json, str) else {}\n",
        "    lh, ls = predict_log_proba_from_counts(counts)\n",
        "    return log_to_prob_pair(lh, ls)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeYGfCYXW89l"
      },
      "source": [
        "Save your prediction probabilities to \"train-predictions.tsv\" with columns Message ID, ham and spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kCKrHbpqZ1gY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved train-predictions.tsv (rows=32542) with columns: Message ID, ham, spam\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import pandas as pd\n",
        "\n",
        "train_feats = pd.read_csv(\"train-features.tsv\", sep=\"\\t\")\n",
        "assert {\"Message ID\",\"features_json\"}.issubset(train_feats.columns)\n",
        "\n",
        "ph_list, ps_list = [], []\n",
        "for s in train_feats[\"features_json\"].astype(str):\n",
        "    ph, ps = predict_proba_features_json(s)\n",
        "    ph_list.append(ph)\n",
        "    ps_list.append(ps)\n",
        "\n",
        "out_train = pd.DataFrame({\n",
        "    \"Message ID\": train_feats[\"Message ID\"],\n",
        "    \"ham\":  ph_list,\n",
        "    \"spam\": ps_list,\n",
        "})\n",
        "\n",
        "out_train.to_csv(\"train-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "print(f\"Saved train-predictions.tsv (rows={len(out_train)}) with columns: Message ID, ham, spam\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGHYjWN9Z3Sq"
      },
      "source": [
        "Submit \"train-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpTlyFLDOCDj"
      },
      "source": [
        "## Part 5: Predict Spam Probability for Test Data\n",
        "\n",
        "Use your previous classifier to predict spam probability for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UELHs9CzXaz1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed probabilities for 1174 test messages.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Message ID</th>\n",
              "      <th>ham</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2242578690</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.508288e-28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1571733090</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.387113e-15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2096463480</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.146677e-11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3889271820</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.490784e-16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2790264210</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.469803e-177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Message ID  ham           spam\n",
              "0  2242578690  1.0   1.508288e-28\n",
              "1  1571733090  1.0   4.387113e-15\n",
              "2  2096463480  1.0   1.146677e-11\n",
              "3  3889271820  1.0   1.490784e-16\n",
              "4  2790264210  1.0  8.469803e-177"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sanity checks\n",
        "assert 'predict_proba_features_json' in globals(), \"Define predict_proba_features_json from Part 4 first.\"\n",
        "test_feats = pd.read_csv(\"test-features.tsv\", sep=\"\\t\")\n",
        "assert {\"Message ID\",\"features_json\"}.issubset(test_feats.columns), \"test-features.tsv missing required columns.\"\n",
        "\n",
        "# Predict\n",
        "test_ph, test_ps = [], []\n",
        "for s in test_feats[\"features_json\"].astype(str):\n",
        "    ph, ps = predict_proba_features_json(s)\n",
        "    test_ph.append(ph)\n",
        "    test_ps.append(ps)\n",
        "\n",
        "# Stash in memory for the save cell\n",
        "test_pred_df = pd.DataFrame({\n",
        "    \"Message ID\": test_feats[\"Message ID\"],\n",
        "    \"ham\":  test_ph,\n",
        "    \"spam\": test_ps,\n",
        "})\n",
        "print(f\"Computed probabilities for {len(test_pred_df)} test messages.\")\n",
        "test_pred_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opc86JSEaAQM"
      },
      "source": [
        "Save your prediction probabilities in \"test-predictions.tsv\" with the same columns as \"train-predictions.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qIg1XaY_Z_Rr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved test-predictions.tsv with columns: Message ID, ham, spam\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "assert 'test_pred_df' in globals(), \"Run the Part 5 prediction cell first.\"\n",
        "test_pred_df.to_csv(\"test-predictions.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved test-predictions.tsv with columns: Message ID, ham, spam\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLLbyE8paGqM"
      },
      "source": [
        "Submit \"test-predictions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wU6ReUMsZNZ8"
      },
      "source": [
        "## Part 6: Construct ROC Curve\n",
        "\n",
        "For every probability threshold from 0.01 to .99 in increments of 0.01, compute the false and true positive rates from the test data using the spam class for positives.\n",
        "That is, if the predicted spam probability is greater than or equal to the threshold, predict spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QAx9jbDBYOVo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   threshold  false_positive_rate  true_positive_rate\n",
            "0       0.01             0.023333            0.993031\n",
            "1       0.02             0.020000            0.993031\n",
            "2       0.03             0.020000            0.993031\n",
            "3       0.04             0.020000            0.993031\n",
            "4       0.05             0.020000            0.993031 \n",
            "...\n",
            "     threshold  false_positive_rate  true_positive_rate\n",
            "94       0.95                 0.01            0.984321\n",
            "95       0.96                 0.01            0.984321\n",
            "96       0.97                 0.01            0.984321\n",
            "97       0.98                 0.01            0.982578\n",
            "98       0.99                 0.01            0.979094\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import pandas as pd, numpy as np, zipfile, io, hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "# 1) Load test predictions (or reuse in-memory df)\n",
        "if 'test_pred_df' in globals():\n",
        "    preds = test_pred_df.copy()\n",
        "else:\n",
        "    preds = pd.read_csv(\"test-predictions.tsv\", sep=\"\\t\")\n",
        "assert {\"Message ID\",\"spam\"}.issubset(preds.columns), \"test-predictions.tsv must have columns Message ID, spam.\"\n",
        "\n",
        "# 2) Load labels from the original CSV inside enron_spam_data.zip and synthesize the same Message ID\n",
        "def _load_enron_zip(zip_path=Path(\"enron_spam_data.zip\")) -> pd.DataFrame:\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        inner = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
        "        assert inner, \"CSV not found inside enron_spam_data.zip\"\n",
        "        with z.open(inner[0]) as f:\n",
        "            df = pd.read_csv(io.TextIOWrapper(f, encoding=\"utf-8\"))\n",
        "    ren = {c.lower(): c for c in df.columns}\n",
        "    df = df.rename(columns={\n",
        "        ren[\"subject\"]:\"Subject\",\n",
        "        ren[\"message\"]:\"Message\",\n",
        "        ren[\"spam/ham\"]:\"Spam/Ham\",\n",
        "        ren[\"date\"]:\"Date\",\n",
        "    })[[\"Subject\",\"Message\",\"Spam/Ham\",\"Date\"]]\n",
        "    return df\n",
        "\n",
        "def _ensure_message_id(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if \"Message ID\" in df.columns:\n",
        "        return df\n",
        "    def mk_id(row):\n",
        "        key = (str(row[\"Subject\"]) + \"\\n\" + str(row[\"Message\"]) + \"\\n\" + str(row[\"Date\"])).encode(\"utf-8\",\"ignore\")\n",
        "        return int(hashlib.md5(key).hexdigest()[:8], 16)\n",
        "    out = df.copy()\n",
        "    out[\"Message ID\"] = out.apply(mk_id, axis=1)\n",
        "    return out\n",
        "\n",
        "raw = _ensure_message_id(_load_enron_zip())\n",
        "label_map = raw.set_index(\"Message ID\")[\"Spam/Ham\"].to_dict()\n",
        "\n",
        "# 3) Attach ground-truth labels to test rows\n",
        "preds[\"Label\"] = preds[\"Message ID\"].map(label_map)\n",
        "preds = preds.dropna(subset=[\"Label\"]).reset_index(drop=True)\n",
        "\n",
        "y_true = (preds[\"Label\"].str.lower() == \"spam\").astype(int).to_numpy()\n",
        "p_spam = preds[\"spam\"].astype(float).to_numpy()\n",
        "\n",
        "# 4) Sweep thresholds\n",
        "ths = np.round(np.linspace(0.01, 0.99, 99), 2)\n",
        "roc_rows = []\n",
        "pos = (y_true == 1).sum()\n",
        "neg = (y_true == 0).sum()\n",
        "pos = max(pos, 1)\n",
        "neg = max(neg, 1)\n",
        "\n",
        "for t in ths:\n",
        "    y_hat = (p_spam >= t).astype(int)\n",
        "    tp = int(((y_true == 1) & (y_hat == 1)).sum())\n",
        "    fp = int(((y_true == 0) & (y_hat == 1)).sum())\n",
        "    fn = int(((y_true == 1) & (y_hat == 0)).sum())\n",
        "    tn = int(((y_true == 0) & (y_hat == 0)).sum())\n",
        "    tpr = tp / pos\n",
        "    fpr = fp / neg\n",
        "    roc_rows.append({\"threshold\": float(t), \"false_positive_rate\": float(fpr), \"true_positive_rate\": float(tpr)})\n",
        "\n",
        "roc_df = pd.DataFrame(roc_rows)\n",
        "print(roc_df.head(), \"\\n...\\n\", roc_df.tail())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baGaDOauX2vE"
      },
      "source": [
        "Save this data in a file \"roc.tsv\" with columns threshold, false_positive_rate and true_positive rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eSHCzA85YP_I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved roc.tsv with columns: threshold, false_positive_rate, true_positive_rate\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "assert 'roc_df' in globals(), \"Run the ROC compute cell first.\"\n",
        "roc_df.to_csv(\"roc.tsv\", sep=\"\\t\", index=False)\n",
        "print(\"Saved roc.tsv with columns: threshold, false_positive_rate, true_positive_rate\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4po8_NMYRuo"
      },
      "source": [
        "Submit \"roc.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynaBbiCZhMYi"
      },
      "source": [
        "## Part 7: Signup for Gemini API Key\n",
        "\n",
        "Create a free Gemini API key at https://aistudio.google.com/app/api-keys.\n",
        "You will need to do this with a personal Google account - it will not work with your BU Google account.\n",
        "This will not incur any charges unless you configure billing information for the key.\n",
        "\n",
        "You will be asked to start a Gemini free trial for week 11.\n",
        "This will not incur any charges unless you exceed expected usage by an order of magnitude.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3xFKcX6hxTL"
      },
      "source": [
        "No submission needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files.\n",
        "You do not need to provide code for data collection if you did that by manually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote acknowledgments.txt \n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Acknowledgements\n",
        "\n",
        "I completed this assignment independently and did not discuss it with anyone.\n",
        "\n",
        "I did not use any generative AI tools for this work.\n",
        "\n",
        "I did not use any additional libraries beyond those provided or referenced in the course materials.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "with open(\"acknowledgments.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(text.strip() + \"\\n\")\n",
        "\n",
        "\n",
        "print(\"Wrote acknowledgments.txt \")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
